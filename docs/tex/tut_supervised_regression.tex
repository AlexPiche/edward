% Define the subtitle of the page
\title{Supervised learning (Regression)}

% Begin the content of the page
\subsection{Supervised learning (Regression)}

In supervised learning, the task is to infer hidden structure from
labeled data, comprised of training examples $\{(x_n, y_n)\}$.
Regression (typically) means the output $y$ takes continuous values.

We demonstrate how to do this in Edward with an example.
The script is available
\href{https://github.com/blei-lab/edward/blob/master/examples/bayesian_linear_regression_test.py}
{here}.


\subsubsection{Data}

Simulate a data set of $40$ data points, which comprises of pairs of inputs $\mathbf{x}_n\in\mathbb{R}^{10}$ and outputs
$y_n\in\mathbb{R}$. They have a linear dependence with normally distributed noise.

\begin{lstlisting}[language=Python]
def build_toy_dataset(N=40, coeff=np.random.randn(10), noise_std=0.1):
    n_dim = len(coeff)
    x = np.random.randn(N, n_dim).astype(np.float32)
    y = np.dot(x, coeff) + norm.rvs(0, noise_std, size=N)
    return {'x': x, 'y': y}

coeff = np.random.randn(10)
data = build_toy_dataset(coeff=coeff)
\end{lstlisting}


\subsubsection{Model}

Posit the model as Bayesian linear regression. For more details on the
model, see the
\href{tut_bayesian_linear_regression.html}
{Bayesian linear regression tutorial}.

Here we build the model in Edward using TensorFlow.
\begin{lstlisting}[language=Python]
class LinearModel:
    """
    Bayesian linear regression for outputs y on inputs x.

    p((x,y), z) = Normal(y | x*z, lik_variance) *
                  Normal(z | 0, prior_variance),

    where z are weights, and with known lik_variance and
    prior_variance.

    Parameters
    ----------
    lik_variance : float, optional
        Variance of the normal likelihood; aka noise parameter,
        homoscedastic variance, scale parameter.
    prior_variance : float, optional
        Variance of the normal prior on weights; aka L2
        regularization parameter, ridge penalty, scale parameter.
    """
    def __init__(self, lik_variance=0.01, prior_variance=0.01):
        self.lik_variance = lik_variance
        self.prior_variance = prior_variance
        self.n_vars = 11

    def log_prob(self, xs, zs):
        """Return a vector [log p(xs, zs[1,:]), ..., log p(xs, zs[S,:])]."""
        x, y = xs['x'], xs['y']
        log_prior = -tf.reduce_sum(zs*zs, 1) / self.prior_variance
        # broadcasting to do (x*W) + b (n_minibatch x n_samples - n_samples)
        b = zs[:, 0]
        W = tf.transpose(zs[:, 1:])
        mus = tf.matmul(x, W) + b
        # broadcasting to do mus - y (n_minibatch x n_samples - n_minibatch x 1)
        y = tf.expand_dims(y, 1)
        log_lik = -tf.reduce_sum(tf.pow(mus - y, 2), 0) / self.lik_variance
        return log_lik + log_prior

    def predict(self, xs, zs):
        """Return a prediction for each data point, averaging over
        each set of latent variables z in zs; and also return the true
        value."""
        x_test = xs['x']
        b = zs[:, 0]
        W = tf.transpose(zs[:, 1:])
        y_pred = tf.reduce_mean(tf.matmul(x_test, W) + b, 1)
        return y_pred

model = LinearModel()
\end{lstlisting}


\subsubsection{Inference}

Perform variational inference.
Define the variational model to be a fully factorized normal
\begin{lstlisting}[language=Python]
variational = Variational()
variational.add(Normal(model.n_vars))
\end{lstlisting}

Run mean-field variational inference for 250 iterations and print
every 10 iterations.
\begin{lstlisting}[language=Python]
inference = ed.MFVI(model, variational, data)
inference.run(n_iter=250, n_print=10)
\end{lstlisting}
In this case \texttt{MFVI} defaults to minimizing the
$\text{KL}(q\|p)$ divergence measure using the reparameterization
gradient.
For more details on inference, see the \href{tut_KLqp.html}{$\text{KL}(q\|p)$ tutorial}.


\subsubsection{Criticism}

Use point-based evaluation, and calculate the mean squared
error for predictions on test data. Here the test data is simulated
from the same process.

\begin{lstlisting}[language=Python]
data_test = build_toy_dataset(coeff=coeff)
x_test, y_test = data_test['x'], data_test['y']
print(ed.evaluate('mse', model, variational, {'x': x_test}, y_test))
## 0.0262413
\end{lstlisting}

The trained model makes predictions with low mean squared error
(relative to the magnitude of the output).

In addition to the model's \texttt{log_prob()} method typically required for
inference, it has a \texttt{predict()} method which makes
predictions. This method is required for point-based evaluation. For
more details on criticism, see the \href{tut_point_eval.html}{point-based
evaluation tutorial}.
