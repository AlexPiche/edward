% Define the subtitle of the page
\title{Probabilistic decoder}

% Begin the content of the page
\subsection{Probabilistic decoder}

A probabilistic decoder is a reinterpretation of model likelihoods
based on coding theory. It is a distribution $p(\mathbf{x}_n\mid \mathbf{z}_n)$  over each value
$\mathbf{x}_n\in\mathbb{R}^D$ given a code $\mathbf{z}_n$. The latent
variables $\mathbf{z}_n$ are interpreted as the hidden representation, or code, of the value
$\mathbf{x}_n$. The decoder is probabilistic because its generated
values (decoding) for any given code is random.

For real-valued data,
the randomness in the decoder is given by a multivariate Gaussian
\begin{align*}
  p(\mathbf{x}_n\mid\mathbf{z}_n)
  &=
  \mathcal{N}(\mathbf{x}_n\mid [\mu,\sigma^2]=\mathrm{NN}(\mathbf{z}_n; \mathbf{\theta})),
\end{align*}
where the probabilistic decoder is parameterized by a neural network
$\mathrm{NN}$ taking the code $\mathbf{z}_n$ as input.

For binary data,
the randomness in the decoder is given by a Bernoulli
\begin{align*}
  p(\mathbf{x}_n\mid\mathbf{z}_n)
  &=
  \text{Bernoulli}(\mathbf{x}_n\mid p=\mathrm{NN}(\mathbf{z}_n; \mathbf{\theta})).
\end{align*}
Probabilistic decoders are typically used alongside a standard normal
prior over the code
\begin{align*}
  p(\mathbf{z})
  &=
  \mathcal{N}(\mathbf{z} \;;\; \mathbf{0}, I).
\end{align*}

Let's build the model in Edward using TensorFlow, and with
PrettyTensor as an easy way to build neural networks. Here we use a
probabilistic decoder to model binarized 28 x 28
pixel images from MNIST.
\begin{lstlisting}[language=Python]
class NormalBernoulli:
    """
    Each binarized pixel in an image is modeled by a Bernoulli
    likelihood. The success probability for each pixel is the output
    of a neural network that takes samples from a normal prior as
    input.

    p(x, z) = Bernoulli(x | p = neural_network(z)) Normal(z; 0, I)
    """
    def __init__(self, n_vars):
        self.n_vars = n_vars # number of local latent variables

    def neural_network(self, z):
        """p = neural_network(z)"""
        with pt.defaults_scope(activation_fn=tf.nn.elu,
                               batch_normalize=True,
                               learned_moments_update_rate=0.0003,
                               variance_epsilon=0.001,
                               scale_after_normalization=True):
            return (pt.wrap(z).
                    reshape([N_MINIBATCH, 1, 1, self.n_vars]).
                    deconv2d(3, 128, edges='VALID').
                    deconv2d(5, 64, edges='VALID').
                    deconv2d(5, 32, stride=2).
                    deconv2d(5, 1, stride=2, activation_fn=tf.nn.sigmoid).
                    flatten()).tensor

    def log_lik(self, xs, z):
        """
        Bernoulli log-likelihood, summing over every image n and pixel i
        in image n.

        log p(x | z) = log Bernoulli(x | p = neural_network(z))
         = sum_{n=1}^N sum_{i=1}^{28*28} log Bernoulli (x_{n,i} | p_{n,i})
        """
        return tf.reduce_sum(bernoulli.logpmf(xs['x'], p=self.neural_network(z)))

model = NormalBernoulli(n_vars=10)
\end{lstlisting}

An example script using this model can found
\href{https://github.com/blei-lab/edward/blob/master/examples/convolutional_vae.py}
{here}.
%We experiment with this model in the
%\href{variational_autoencoder.html}{variational auto-encoder} tutorial.

\subsubsection{Footnotes}

The neural network which parameterizes the probabilistic decoder is
also known as a generative network. It is in analogy to an
\href{tut_inference_networks.html}{inference network}, which
can parameterize a variational model used for inference,
interpreted as a probabilistic encoder.

Traditionally, a probabilistic encoder is the most common
choice of inference. This lead to the coinage of the model-inference
combination known as the variational auto-encoder
(Kingma and Welling, 2014), which is a probabilistic extension of
auto-encoders.
We recommend against this terminology,
in favor of making explicit the separation of model and inference.
That is, probabilistic decoders are a general class of
models that can be used without an encoder.
Variational
inference is not necessary to infer probabilistic decoders, and
variational inference can also be done without an inference network.

\subsubsection{References}\label{references}

\begin{itemize}
\item
  Dayan, P., Hinton, G. E., Neal, R. M., & Zemel, R. S. (1995). The Helmholtz Machine. Neural Computation, 7(5), 889â€“904.
\item
  Kingma, D. P., & Welling, M. (2014). Auto-Encoding Variational Bayes. In International Conference on Learning Representations.
\end{itemize}
