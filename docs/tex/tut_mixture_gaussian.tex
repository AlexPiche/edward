% Define the subtitle of the page
\title{Mixture of Gaussians}

% Begin the content of the page
\subsection{Mixture of Gaussians}

A mixture of Gaussians, also known as a Gaussian mixture model,
clusters data points $\mathbf{x}_n\in\mathbb{R}^D$ using $K$
individual Gaussian distributions.

For a set of $N$ data points,
the likelihood of each observation $\mathbf{x}_n$ is
\begin{align*}
  p(\mathbf{x}_n \mid \pi, \mu, \sigma)
  &=
  \sum_{k=1}^K \pi_k \, \mathcal{N}(\mathbf{x}_n ; \mu_k, \sigma_k).
\end{align*}
The latent variable $\pi$ is a $K$-dimensional probability vector
which mixes individual Gaussian distributions, each
characterized by mean $\mu_k$ and variance $\sigma_k$.

Define the prior on $\pi\in[0,1]$ such that $\sum_{k=1}^K\pi_k=1$ to be
\begin{align*}
  p(\pi)
  &=
  \text{Dirichlet}(\pi \;;\; \alpha \mathbf{1}_{K}).
\end{align*}

Define the prior on each component $\mathbf{\mu}_k\in\mathbb{R}^D$ to be
\begin{align*}
  p(\mathbf{\mu}_k)
  &=
  \mathcal{N}(\mathbf{\mu}_k \;;\; 0, \sigma^2\mathbf{I}).
\end{align*}

Define the prior on each component $\mathbf{\sigma}_k\in\mathbb{R}^D$ to be
\begin{align*}
  p(\mathbf{\sigma}_k)
  &=
  \text{InvGamma}(\mathbf{\sigma}_k \;;\; a, b).
\end{align*}

Let's build the model in Edward using TensorFlow.
\begin{lstlisting}[language=Python]
class MixtureGaussian:
    """
    Mixture of Gaussians

    p(x, z) = [ prod_{n=1}^N N(x_n; mu_{c_n}, sigma_{c_n}) Multinomial(c_n; pi) ]
              [ prod_{k=1}^K N(mu_k; 0, cI) Inv-Gamma(sigma_k; a, b) ]
              Dirichlet(pi; alpha)

    where z = {pi, mu, sigma} and for known hyperparameters a, b, c, alpha.

    Parameters
    ----------
    K : int
        Number of mixture components.
    D : float, optional
        Dimension of the Gaussians.
    """
    def __init__(self, K, D):
        self.K = K
        self.D = D
        self.n_vars = (2*D + 1) * K

        self.a = 1
        self.b = 1
        self.c = 10
        self.alpha = tf.ones([K])

    def log_prob(self, xs, zs):
        """Returns a vector [log p(xs, zs[1,:]), ..., log p(xs, zs[S,:])]."""
        N = get_dims(xs['x'])[0]
        pi, mus, sigmas = zs
        log_prior = dirichlet.logpdf(pi, self.alpha)
        log_prior += tf.reduce_sum(norm.logpdf(mus, 0, np.sqrt(self.c)), 1)
        log_prior += tf.reduce_sum(invgamma.logpdf(sigmas, self.a, self.b), 1)

        # Loop over each sample zs[b,:]
        log_lik = []
        n_samples = get_dims(zs[0])[0]
        for s in range(n_samples):
            log_lik_z = N*tf.reduce_sum(tf.log(pi), 1)
            for k in range(self.K):
                log_lik_z += tf.reduce_sum(multivariate_normal.logpdf(xs['x'],
                    mus[s, (k*self.D):((k+1)*self.D)],
                    sigmas[s, (k*self.D):((k+1)*self.D)]))

            log_lik += [log_lik_z]

        return log_prior + tf.pack(log_lik)

model = MixtureGaussian(K=2, D=2)
\end{lstlisting}

We experiment with this model using variational inference in the
\href{tut_unsupervised.html}{unsupervised learning} tutorial.
Example scripts using this model can found
\href{https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian_map.py}
{here with MAP estimation} and
\href{https://github.com/blei-lab/edward/blob/master/examples/mixture_gaussian_laplace.py}
{here with the Laplace approximation}.

\subsubsection{References}\label{references}

\begin{itemize}
\item
  Bishop, C. (2007). Pattern Recognition and Machine Learning. Springer.
\end{itemize}
